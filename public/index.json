[
{
	"uri": "//localhost:1313/2-prerequiste/2.1-awscli/",
	"title": "Install and configure AWS CLI",
	"tags": [],
	"description": "",
	"content": "1. Install Run the following commands: curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34;\runzip awscliv2.zip\rsudo ./aws/install 2. Configuration Run configuration command: aws configure Enter the following:\nAWS Access Key ID: Obtain from AWS IAM Sign in to the AWS Management Console: Navigate to https://aws.amazon.com/console/. Log in with your AWS account credentials. Navigate to IAM: In the AWS Management Console, search for IAM. Select IAM from the services list to open the IAM dashboard. 3. Select an IAM User:\nIn the dashboard menu, click Users. Click on the IAM user. 4. Create Access Key:\nClick the Security credentials tab. Click Create access key. Select the use case (e.g., Command Line Interface (CLI)). Add an optional description tag for the key (e.g., workshop-key). Click Create access key. On the next screen, you’ll see: Access Key ID. Secret Access Key. Download the .csv file containing these credentials or copy them to a secure location. The Secret Access Key is only shown once and cannot be retrieved later.\nAWS Secret Access Key: Import from CSV file. Default region name: E.g., us-east-1. Default output format: Enter json or press Enter to use the default. The configuration is saved in: %USERPROFILE%\\.aws\\credentials and %USERPROFILE%\\.aws\\config\nTest the configuration by running:\naws sts get-caller-identity "
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Overview This workshop provides a hands-on guide to managing multiple Kubernetes clusters on AWS using Cluster API, enhanced with GitOps deployment via Argo CD. Cluster API offers declarative APIs for automating cluster lifecycle operations, including provisioning, upgrades, scaling, deletion.\nArgo CD integration ensures consistent and auditable application delivery across clusters, aligning with GitOps best practices.\nObjective Automate Kubernetes cluster lifecycle: provisioning, scaling, upgrades, and decommissioning. Manage multiple EKS clusters on AWS using Cluster API. Enable GitOps-based deployment across clusters with Argo CD. Focus on production-grade, scalable multi-cluster architecture. By the end of this workshop, participants will have a robust multi-cluster management setup with automated operations and Git-driven deployment workflows.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-tools/2.2.1-kubectl/",
	"title": "Kubectl (CLI for Kubernet)",
	"tags": [],
	"description": "",
	"content": "Kubernetes provides a command line tool for communicating with a Kubernetes cluster\u0026rsquo;s control plane, using the Kubernetes API. This tool is named kubectl\nInstall Run the following commands: curl -LO \u0026#34;https://dl.k8s.io/release/v1.33.3/bin/linux/amd64/kubectl\u0026#34;\rchmod +x ./kubectl\rsudo mv ./kubectl /usr/local/bin/kubectl Verify installation kubectl version --client "
},
{
	"uri": "//localhost:1313/",
	"title": "Kubernetes Multi-Cluster Management with Cluster API",
	"tags": [],
	"description": "",
	"content": "Kubernetes Multi-Cluster Management with Cluster API Overall This workshop guides participants through implementing multi-cluster Kubernetes management using Cluster API on AWS. Cluster API is a Kubernetes project that provides declarative APIs for cluster lifecycle management, enabling automation of cluster provisioning, upgrades, and scaling across multiple clusters. Participants will learn to:\nObjective Automate Kubernetes cluster lifecycle (creation, scaling, upgrades, and deletion). Set up cross-cluster networking for seamless communication. Distribute workloads across clusters for high availability and scalability. Implement governance and security policies to ensure compliance and secure operations. Apply cost management strategies to optimize AWS resource usage. Define operational procedures for multi-cluster management. The workshop uses AWS EKS (Elastic Kubernetes Service) with Cluster API to manage multiple clusters, focusing on practical, hands-on implementation. By the end, participants will have a functional multi-cluster setup with automated lifecycle management, secure networking, and cost-optimized operations.\nContent Introduction Preparation Initialize Cluster API Management Cluster Create Workload Clusters Automation Upgrade and Auto-scaling Clean up resources "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-tools/2.2.2-eksctl/",
	"title": "Eksctl (CLI for Amazon EKS)",
	"tags": [],
	"description": "",
	"content": "eksctl is a command-line tool for creating and managing Kubernetes clusters on Amazon EKS. It simplifies cluster management tasks using the AWS API.\nInstall Run the following commands: curl --silent --location \u0026#34;https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp\rsudo mv /tmp/eksctl /usr/local/bin/\rsudo chmod +x /usr/local/bin/eksctl Verify installation Run the following command in Command Prompt or PowerShell:\neksctl version "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-tools/",
	"title": "Install tools",
	"tags": [],
	"description": "",
	"content": "Content Kubectl (CLI for Kubernet) Eksctl (CLI for Amazon EKS) Clusterctl (CLI for Cluster API) Clusterawsadm Kind Docker Additional Resources AWS IAM AWS CLI Configuration kubectl eksctl clusterctl AWS EKS "
},
{
	"uri": "//localhost:1313/2-prerequiste/",
	"title": "Preparation ",
	"tags": [],
	"description": "",
	"content": "\rYou need to set up the required tools and AWS environment to perform this lab.\nContent Install and configure AWS CLI Install tools "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-tools/2.2.3-clusterctl/",
	"title": "Clusterctl (CLI for Cluster API)",
	"tags": [],
	"description": "",
	"content": "clusterctl is a command-line tool for managing Kubernetes clusters using the Cluster API. It simplifies the creation, upgrade, and deletion of clusters by interacting with Cluster API providers.\nInstall Run the following commands: curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.9.3/clusterctl-linux-amd64 -o clusterctl\rchmod +x clusterctl\rsudo install -o root -g root -m 0755 clusterctl /usr/local/bin/clusterctl Verify Installation Test to ensure the version you installed is up-to-date:\nclusterctl version "
},
{
	"uri": "//localhost:1313/3-iniclusters/",
	"title": "Initialize Cluster API Management Cluster",
	"tags": [],
	"description": "",
	"content": "1️⃣ Fork and Clone the Git Repository Start by forking the official sample repository eks-ec2-clusterapi-gitops to your own GitHub account. Then, clone it to your local machine:\ncd ./eks-ec2-clusterapi-gitops This repository contains configuration files and manifests sample to help you deploy EKS clusters and Cluster EC2 using Cluster API and GitOps workflows.\n2️⃣ Create the Management Cluster with Kind Use Kind to create a Kubernetes cluster locally. This cluster will serve as your management cluster, which controls the lifecycle of other clusters:\nkind create cluster\rkubectl cluster-info --context kind-kind The second command verifies that the cluster was created successfully and is accessible.\n3️⃣ Bootstrap IAM Resources for AWS Provider Cluster API for AWS requires specific IAM roles and policies to interact with AWS services. The clusterawsadm tool helps you create these resources via a CloudFormation stack:\nclusterawsadm bootstrap iam create-cloudformation-stack --region us-east-1 This stack sets up the necessary permissions so that your management cluster can create and manage EKS workload clusters.\n4️⃣ Set Required Environment Variables Before initializing Cluster API, you need to define several environment variables that provide credentials and enable specific features:\nexport AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile)\rexport AWS_REGION=us-east-1\rexport EKS=true\rexport EXP_MACHINE_POOL=true\rexport CAPA_EKS_IAM=true AWS_B64ENCODED_CREDENTIALS: Encoded AWS credentials for Cluster API. EKS=true: Enables EKS support. EXP_MACHINE_POOL=true: Enables experimental support for MachinePools. CAPA_EKS_IAM=true: Enables IAM integration for EKS clusters. 5️⃣ Initialize Cluster API with AWS Infrastructure Provider Now, use the clusterctl init command to install Cluster API components into your Kind cluster. This transforms it into a fully functional management cluster:\nclusterctl init --infrastructure aws This command installs:\nInfrastructure provider: cluster-api-provider-aws Bootstrap provider: kubeadm Control plane provider: kubeadm Once completed, your management cluster is ready to create and manage EKS clusters on AWS using declarative APIs.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-tools/2.2.4.clusterawsadm/",
	"title": "Clusterawsadm",
	"tags": [],
	"description": "",
	"content": "clusterawsadm provides helpers for bootstrapping Kubernetes Cluster API Provider AWS. Use clusterawsadm to view required AWS Identity and Access Management (IAM) policies as JSON docs, or create IAM roles and instance profiles automatically using AWS CloudFormation.\nclusterawsadm additionally helps provide credentials for use with clusterctl.\nInstall Run the following commands: curl -LO https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.8.4/clusterawsadm-linux-amd64\rchmod +x clusterawsadm-linux-amd64\rsudo install -o root -g root -m 0755 clusterawsadm-linux-amd64 /usr/local/bin/clusterawsadm Verify Installation Test to ensure the version you installed is up-to-date:\nclusterawsadm version "
},
{
	"uri": "//localhost:1313/4-creclusters/",
	"title": "Create Workload Cluster",
	"tags": [],
	"description": "",
	"content": "Create Workload Clusters on EC2 and EKS Generate SSH Keys Cluster API requires SSH keys to access EC2 nodes. You need to create separate keys for each region:\naws ec2 create-key-pair --key-name capi-eks --region ap-southeast-2 --query \u0026#39;KeyMaterial\u0026#39; --output text \u0026gt; capi-eks.pem\raws ec2 create-key-pair --key-name capi-ec2 --region us-east-1 --query \u0026#39;KeyMaterial\u0026#39; --output text \u0026gt; capi-ec2.pem Note: EC2 and EKS clusters are deployed in different regions, so each requires its own key.\nDefine Environment Variables export AWS_CONTROL_PLANE_MACHINE_TYPE=t3.medium\rexport AWS_NODE_MACHINE_TYPE=t3.medium These variables specify the EC2 instance types for control plane and worker nodes.\nGenerate EC2 Cluster Manifest export AWS_SSH_KEY_NAME=capi-ec2\rexport AWS_REGION=us-east-1\rclusterctl generate cluster capi-ec2 --kubernetes-version v1.31.0 --control-plane-machine-count=3 --worker-machine-count=3 \u0026gt; ./capi-cluster/aws-ec2/aws-ec2.yaml This command generates a YAML manifest for a Kubernetes cluster running on EC2 with 3 control plane nodes and 3 worker nodes.\nGenerate EKS Cluster Manifest export AWS_SSH_KEY_NAME=capi-eks\rexport AWS_REGION=ap-southeast-2\rclusterctl generate cluster capi-eks --flavor eks-managedmachinepool --kubernetes-version v1.31.0 --worker-machine-count=3 \u0026gt; ./capi-cluster/aws-eks/capi-eks.yaml The eks-managedmachinepool flavor is used to deploy an EKS cluster with managed node groups.\nInstall Argo CD on the Management Cluster Install Argo CD on the Kind-based management cluster:\nkubectl apply -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml Retrieve the initial admin password:\nkubectl get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d; echo Access the Argo CD web UI:\nkubectl port-forward svc/argocd-server 8080:80 Username: admin Password: (use the password retrieved above) Connect your Git repository via Settings \u0026gt; Repositories \u0026gt; Connect repo.\nGrant Permissions to Argo CD To allow Argo CD to manage Cluster API resources, grant it cluster-admin permissions:\nkubectl apply -f ./management/argo-cluster-role-binding.yaml Deploy Clusters via Argo CD Argo CD uses Application manifests to deploy clusters. Example for EC2:\napiVersion: argoproj.io/v1alpha1\rkind: Application\rmetadata:\rname: ec2-cluster\rspec:\rdestination:\rnamespace: default\rserver: https://kubernetes.default.svc\rsource:\rpath: capi-cluster/aws-ec2\rrepoURL: [update the Git Repo accordingly]\rtargetRevision: HEAD\rproject: default\rsyncPolicy:\rautomated:\rprune: true\rallowEmpty: true After updating the manifest, commit and push to Git:\ngit add .\rgit commit -m \u0026#34;Update Argo CD app manifest\u0026#34;\rgit push Apply the manifest to create the EC2 cluster:\nkubectl apply -f ./management/argocd-ec2-app.yaml Similarly, deploy the EKS cluster:\nkubectl apply -f ./management/argocd-eks-app.yaml Check Cluster Status clusterctl describe cluster capi-eks\rclusterctl describe cluster capi-ec2 If the EC2 cluster shows READY=False, it may be missing a CNI. Deploy Calico to enable node networking:\nclusterctl get kubeconfig capi-ec2 \u0026gt; capikubeconfig-ec2\rkubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml Retrieve Kubeconfig for EKS Cluster Unlike EC2 clusters, EKS kubeconfig is stored as a secret in the management cluster:\nkubectl --namespace=default get secret capi-eks-user-kubeconfig -o jsonpath={.data.value} | base64 --decode \u0026gt; capikubeconfig-eks "
},
{
	"uri": "//localhost:1313/5-upgrade_scaling/",
	"title": "Automation Upgrade and Auto-scaling",
	"tags": [],
	"description": "",
	"content": "Cluster Upgrades Upgrading a Kubernetes cluster managed by Cluster API is a declarative process. You simply update the version fields in your cluster manifests and let Argo CD synchronize the changes automatically.\nChoose the target Kubernetes version\nFor example: v1.32.0. Make sure the version is compatible with your infrastructure and supported by Cluster API.\nUpdate your cluster manifest files\nFor EC2-based clusters (aws-ec2.yaml): Locate the KubeadmControlPlane and MachineDeployment resources in the manifest.\nUpdate the version field to the desired Kubernetes version.\nEnsure MachineDeployment uses a rolling update strategy to avoid downtime:\nstrategy:\rtype: RollingUpdate\rrollingUpdate:\rmaxSurge: 1\rmaxUnavailable: 0 This configuration ensures that only one new node is added at a time, and no existing node is taken offline during the update.\nCommit and push the changes to your Git repository.\nArgo CD will detect the changes and apply them automatically. You can monitor the sync status in the Argo CD UI.\nFor EKS clusters (capi-eks.yaml): Locate the AWSManagedControlPlane resource. Update the version field to the new Kubernetes version. Commit and push the changes. Argo CD will synchronize the update and apply it to the EKS cluster. Note: This upgrade method ensures consistency, traceability, and automation through GitOps.\nCluster Autoscaling Autoscaling allows your cluster to dynamically adjust the number of nodes based on resource usage, improving cost efficiency and performance.\nEC2-Based Cluster Autoscaling 1. Create Cluster Autoscaler Deployment Create a manifest file management/cluster-autoscaler.yaml that defines the Cluster Autoscaler deployment:\nUses the official image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.32.0 Targets the EC2 node group: --nodes=1:10:capi-ec2-md-0 Sets the cluster name: --cluster-name=capi-ec2 Includes flags for balancing, logging, and safe eviction This deployment enables the autoscaler to scale the node group between 1 and 10 nodes based on demand.\n2. Define RBAC Permissions Create management/cluster-autoscaler-rbac.yaml to grant the autoscaler access to necessary resources:\nServiceAccount: cluster-autoscaler ClusterRole: allows reading and watching pods, nodes, machine deployments, etc. ClusterRoleBinding: binds the role to the service account Without these permissions, the autoscaler cannot monitor or modify cluster resources.\n3. Deploy via Argo CD Create an Argo CD Application manifest (management/argocd-autoscaler-app.yaml) to automate deployment of the autoscaler and its RBAC configuration:\nkubectl apply -f management/argocd-autoscaler-app.yaml Once applied, Argo CD will manage the autoscaler and keep it in sync with your Git repository.\nEKS Cluster Autoscaling For EKS clusters, autoscaling is typically handled via Managed Node Groups, which are natively supported by AWS.\nConfigure Managed Node Group In your EKS manifest (AWSManagedMachinePool), define the scaling parameters:\nspec:\rinstanceType: t3.medium\rscaling:\rmaxSize: 5\rminSize: 1 This configuration allows AWS to automatically adjust the number of nodes between 1 and 5 based on workload.\nAlternative: Use Cluster Autoscaler for Unmanaged Node Groups If you\u0026rsquo;re using unmanaged node groups in EKS, you can deploy the Cluster Autoscaler manually using the same approach as the EC2 cluster.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-tools/2.2.5-kind/",
	"title": "Kind",
	"tags": [],
	"description": "",
	"content": "Install Run the following commands: curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.24.0/kind-linux-amd64\rchmod +x ./kind\rsudo install -o root -g root -m 0755 ./kind /usr/local/bin/kind Verify Installation kind version "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-tools/2.2.6-docker/",
	"title": "Docker",
	"tags": [],
	"description": "",
	"content": "Install Run the following commands: # Update the package index\rsudo apt-get update\r# Install required packages for HTTPS repository access\rsudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common\r# Add Docker\u0026#39;s official GPG key\rcurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\r# Add Docker\u0026#39;s official APT repository\recho \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null\r# Update the package index again\rsudo apt-get update\r# Install Docker\rsudo apt-get install -y docker-ce docker-ce-cli containerd.io\r# Start and enable Docker service\rsudo systemctl start docker\rsudo systemctl enable docker Verify Installation docker --version "
},
{
	"uri": "//localhost:1313/6-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "To avoid unnecessary AWS charges and ensure a clean resources after completing the workshop, follow these steps to remove all provisioned resources.\nDelete the Argo CD applications: These applications were used to deploy clusters and workloads. Removing them ensures Argo CD no longer manages or syncs related resources.\nkubectl delete -f ./management/argocd-ec2-app.yaml kubectl delete -f ./management/argocd-eks-app.yaml Delete the created clusters: kubectl delete cluster capi-eks\rkubectl delete cluster capi-ec2 Delete the created EC2 SSH key in each region: aws ec2 delete-key-pair --key-name capi-eks --region ap-southeast-2\raws ec2 delete-key-pair --key-name capi-ec2 --region us-east-1 Delete IAM Resources via CloudFormation When initializing Cluster API for AWS, a CloudFormation stack was created to provision IAM roles and policies. You should delete this stack to clean up those resources.\naws cloudformation delete-stack --stack-name cluster-api-provider-aws-sigs-k8s-io --region us-east-1 Delete management cluster kind delete cluster "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]