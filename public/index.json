[
{
	"uri": "//localhost:1313/2-prerequiste/2.1-awscli/",
	"title": "Install and configure AWS CLI",
	"tags": [],
	"description": "",
	"content": "1. Install Run the following commands: curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34;\runzip awscliv2.zip\rsudo ./aws/install 2. Configuration Run configuration command: aws configure Enter the following:\nAWS Access Key ID: Obtain from AWS IAM Sign in to the AWS Management Console: Navigate to https://aws.amazon.com/console/. Log in with your AWS account credentials. Navigate to IAM: In the AWS Management Console, search for IAM. Select IAM from the services list to open the IAM dashboard. 3. Select an IAM User:\nIn the dashboard menu, click Users. Click on the IAM user. 4. Create Access Key:\nClick the Security credentials tab. Click Create access key. Select the use case (e.g., Command Line Interface (CLI)). Add an optional description tag for the key (e.g., workshop-key). Click Create access key. On the next screen, you’ll see: Access Key ID. Secret Access Key. Download the .csv file containing these credentials or copy them to a secure location. The Secret Access Key is only shown once and cannot be retrieved later.\nAWS Secret Access Key: Import from CSV file. Default region name: E.g., us-east-1. Default output format: Enter json or press Enter to use the default. The configuration is saved in: %USERPROFILE%\\.aws\\credentials and %USERPROFILE%\\.aws\\config\nTest the configuration by running:\naws sts get-caller-identity Additional Resources AWS IAM Documentation: https://docs.aws.amazon.com/iam/ AWS CLI Configuration Guide: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html "
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Overview This workshop guides participants through implementing multi-cluster Kubernetes management using Cluster API on AWS. Cluster API is a Kubernetes project that provides declarative APIs for cluster lifecycle management, enabling automation of cluster provisioning, upgrades, and scaling across multiple clusters.\nObjective Automate Kubernetes cluster lifecycle (creation, scaling, upgrades, and deletion). Set up cross-cluster networking for seamless communication. Distribute workloads across clusters for high availability and scalability. Implement governance and security policies to ensure compliance and secure operations. Apply cost management strategies to optimize AWS resource usage. Define operational procedures for multi-cluster management. The workshop uses AWS EKS (Elastic Kubernetes Service) with Cluster API to manage multiple clusters, focusing on practical, hands-on implementation. By the end, participants will have a functional multi-cluster setup with automated lifecycle management, secure networking, and cost-optimized operations.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-tools/2.2.1-kubectl/",
	"title": "Kubectl (CLI for Kubernet)",
	"tags": [],
	"description": "",
	"content": "Kubernetes provides a command line tool for communicating with a Kubernetes cluster\u0026rsquo;s control plane, using the Kubernetes API. This tool is named kubectl\nInstall Run the following commands: curl -LO \u0026#34;https://dl.k8s.io/release/v1.33.3/bin/linux/amd64/kubectl\u0026#34;\rchmod +x ./kubectl\rsudo mv ./kubectl /usr/local/bin/kubectl Verify installation kubectl version --client "
},
{
	"uri": "//localhost:1313/",
	"title": "Kubernetes Multi-Cluster Management with Cluster API",
	"tags": [],
	"description": "",
	"content": "Kubernetes Multi-Cluster Management with Cluster API Overall This workshop guides participants through implementing multi-cluster Kubernetes management using Cluster API on AWS. Cluster API is a Kubernetes project that provides declarative APIs for cluster lifecycle management, enabling automation of cluster provisioning, upgrades, and scaling across multiple clusters. Participants will learn to:\nObjective Automate Kubernetes cluster lifecycle (creation, scaling, upgrades, and deletion). Set up cross-cluster networking for seamless communication. Distribute workloads across clusters for high availability and scalability. Implement governance and security policies to ensure compliance and secure operations. Apply cost management strategies to optimize AWS resource usage. Define operational procedures for multi-cluster management. The workshop uses AWS EKS (Elastic Kubernetes Service) with Cluster API to manage multiple clusters, focusing on practical, hands-on implementation. By the end, participants will have a functional multi-cluster setup with automated lifecycle management, secure networking, and cost-optimized operations.\nContent Introduction Preparation Initialize Cluster API Management Cluster Create Workload Clusters Set Up Cross-Cluster Networking Workload Distribution Implement Governance and Security Cost Management Operational Procedures Clean up resources "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-tools/2.2.2-eksctl/",
	"title": "Eksctl (CLI for Amazon EKS)",
	"tags": [],
	"description": "",
	"content": "eksctl is a command-line tool for creating and managing Kubernetes clusters on Amazon EKS. It simplifies cluster management tasks using the AWS API.\nInstall Run the following commands: curl --silent --location \u0026#34;https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp\rsudo mv /tmp/eksctl /usr/local/bin/\rsudo chmod +x /usr/local/bin/eksctl Verify installation Run the following command in Command Prompt or PowerShell:\neksctl version "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-tools/",
	"title": "Install tools",
	"tags": [],
	"description": "",
	"content": "Content Kubectl (CLI for Kubernet) Eksctl (CLI for Amazon EKS) Clusterctl (CLI for Cluster API) Additional Resources kubectl eksctl clusterctl AWS EKS "
},
{
	"uri": "//localhost:1313/2-prerequiste/",
	"title": "Preparation ",
	"tags": [],
	"description": "",
	"content": "\rYou need to set up the required tools and AWS environment to perform this lab.\nContent Install and configure AWS CLI Install tools Create EC2 Keypair Create S3 Bucket "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-tools/2.2.3-clusterctl/",
	"title": "Clusterctl (CLI for Cluster API)",
	"tags": [],
	"description": "",
	"content": "clusterctl is a command-line tool for managing Kubernetes clusters using the Cluster API. It simplifies the creation, upgrade, and deletion of clusters by interacting with Cluster API providers.\nInstall Run the following commands: curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.9.3/clusterctl-linux-amd64 -o clusterctl\rchmod +x clusterctl\rsudo install -o root -g root -m 0755 clusterctl /usr/local/bin/clusterctl Verify Installation Test to ensure the version you installed is up-to-date:\nclusterctl version "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.3-creec2keypair/",
	"title": "Create EC2 Keypair",
	"tags": [],
	"description": "",
	"content": " Create the SSH key pair: Run the following command to create an SSH key pair named workshop-key in the us-east-1 region:\naws ec2 create-key-pair --key-name workshop-key --query \u0026#39;KeyMaterial\u0026#39; --output text \u0026gt; workshop-key.pem Verify the key pair in AWS Console:\nSign in to the AWS Management Console. Navigate to EC2 → Key Pairs. Look for the workshop-key in the us-east-1 region. "
},
{
	"uri": "//localhost:1313/3-iniclusters/",
	"title": "Initialize Cluster API Management Cluster",
	"tags": [],
	"description": "",
	"content": "1. Fork the git repo Fork the Git repository https://github.com/aws-samples/eks-ec2-clusterapi-gitops to your own Github account, clone to your local workstation, then change to the following directory\ncd ./eks-ec2-clusterapi-gitops 2. Initialize the management cluster Create the kind cluster and verify the success of the creation:\nkind create cluster\rkubectl cluster-info --context kind-kind The Cluster API Provider for AWS includes clusterawsadm, a tool to manage IAM objects. It uses environment variables, encodes them into a Kubernetes Secret in a Kind cluster, and retrieves necessary permissions to create workload clusters.\nclusterawsadm bootstrap iam create-cloudformation-stack --region us-east-1 3. Predefine all necessary environment parameters export AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile)\rexport AWS_REGION=us-east-1\rexport EKS=true\rexport EXP_MACHINE_POOL=true\rexport CAPA_EKS_IAM=true 4. Initialize Cluster API with AWS provider To install the Cluster API components for AWS, the kubeadm boostrap provider, and the kubeadm control-plane provider, and transform the Kind cluster into a management cluster, we use the clusterctl init command.\nclusterctl init --infrastructure aws "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-tools/2.2.4.clusterawsadm/",
	"title": "Clusterawsadm",
	"tags": [],
	"description": "",
	"content": "clusterawsadm provides helpers for bootstrapping Kubernetes Cluster API Provider AWS. Use clusterawsadm to view required AWS Identity and Access Management (IAM) policies as JSON docs, or create IAM roles and instance profiles automatically using AWS CloudFormation.\nclusterawsadm additionally helps provide credentials for use with clusterctl.\nInstall Run the following commands: curl -LO https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.8.4/clusterawsadm-linux-amd64\rchmod +x clusterawsadm-linux-amd64\rsudo install -o root -g root -m 0755 clusterawsadm-linux-amd64 /usr/local/bin/clusterawsadm Verify Installation Test to ensure the version you installed is up-to-date:\nclusterawsadm version "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.4-cres3/",
	"title": "Create S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Create an S3 Bucket for Cluster API State:\nCreate an S3 bucket to store Cluster API state: aws s3 mb s3://capi-workshop-state --region us-east-1 Verify: aws s3 ls s3://capi-workshop-state The bucket name must be globally unique. If capi-workshop-state is taken, append a unique suffix.\n"
},
{
	"uri": "//localhost:1313/4-creclusters/",
	"title": "Create Workload Cluster",
	"tags": [],
	"description": "",
	"content": "Generate the Kubernetes Cluster Manifests Create the SSH key: aws ec2 create-key-pair --key-name capi-eks --region ap-southeast-2 --query \u0026#39;KeyMaterial\u0026#39; --output text \u0026gt; capi-eks.pem\raws ec2 create-key-pair --key-name capi-ec2 --region us-east-1 --query \u0026#39;KeyMaterial\u0026#39; --output text \u0026gt; capi-ec2.pem Define environment variables: export AWS_CONTROL_PLANE_MACHINE_TYPE=t3.medium\rexport AWS_NODE_MACHINE_TYPE=t3.medium Generate the Kubernetes Cluster Manifest for EC2: export AWS_SSH_KEY_NAME=capi-ec2 (Given we are deploying the EC2 cluster and EKS cluster in different regions)\rexport AWS_REGION=us-east-1 clusterctl generate cluster capi-ec2 --kubernetes-version v1.31.0 --control-plane-machine-count=3 --worker-machine-count=3 \u0026gt; ./capi-cluster/aws-ec2/aws-ec2.yaml Generate the Kubernetes Cluster Manifest for EKS: export AWS_SSH_KEY_NAME=capi-eks\rexport AWS_REGION=ap-southeast-2\rclusterctl generate cluster capi-eks --flavor eks-managedmachinepool --kubernetes-version v1.31.0 --worker-machine-count=3 \u0026gt; ./capi-cluster/aws-eks/capi-eks.yaml Install Argo CD on the management Kubernetes cluster install Argo CD in the Kind management cluster:\nkubectl apply -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml Get the first login password to Argo CD Web UI:\nkubectl get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d; echo Set up Argo CD can be authenticated to Git\nLogin Argo CD’s web portal:\nkubectl port-forward svc/argocd-server 8080:80 Username: admin password: first login password Navigate setting \u0026gt; Repositories \u0026gt; Connect repo\nChoose your connection method: HTTP/HTTPS Type: Git Repository URL: Your url fork repo Select skip server verification Connect\nCreate Workload Clusters To authorize Argo CD to manage Cluster API objects, create a ClusterRoleBinding to grant the necessary permissions to the argocd-application-controller ServiceAccount, using the cluster-admin role for simplicity.\nkubectl apply -f ./management/argo-cluster-role-binding.yaml apiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRoleBinding\rmetadata:\rname: cluster-admin-argocd-contoller\rsubjects:\r- kind: ServiceAccount\rname: argocd-application-controller\rnamespace: default\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: ClusterRole\rname: cluster-admin Create Clusters for K8S cluster that runs on EC2: To create a new EKS cluster or a K8S cluster that runs on EC2, we can first define an application in a declarative approach, as a representation of a collection of Kubernetes manifests that makes up all the pieces to deploy the new cluster. In this guide, all of the configuration of the Argo CD application to be added is stored in “Management” folder of the cloned repository. For example, below is the application manifest file for creating a new K8S cluster that runs on EC2:\napiVersion: argoproj.io/v1alpha1\rkind: Application\rmetadata:\rname: ec2-cluster\rspec:\rdestination:\rname: \u0026#39;\u0026#39;\rnamespace: \u0026#39;default\u0026#39;\rserver: \u0026#39;https://kubernetes.default.svc\u0026#39;\rsource:\rpath: capi-cluster/aws-ec2\rrepoURL: \u0026#39;[update the Git Repo accordingly]\u0026#39; #Indicate which source repo for fetching the cluster configuration\rtargetRevision: HEAD\rproject: default #You can give a project name here\rsyncPolicy:\rautomated:\rprune: true\rallowEmpty: true After modifying the Argo CD application yaml file as above, commit and push the updates to the source repo:\ngit push .\rgit add .\rgit commit -m “updates Argo CD app manifest file” Run the following command to create a new Application to create the K8S cluster that runs on EC2:\nkubectl apply -f ./management/argocd-ec2-app.yaml We will use an Argo CD application to provision the EKS cluster. First, create an application and update the relevant YAML file to set the repoURL to your Git repository. Then, commit and push the changes to the source repository.\nkubectl apply -f ./management/argocd-eks-app.yaml Check status of the clusters\nclusterctl describe cluster capi-eks\rclusterctl describe cluster capi-ec2 The EC2 cluster\u0026rsquo;s worker nodes show a \u0026ldquo;False\u0026rdquo; READY status due to the absence of a CNI, preventing them from joining the cluster. For demonstration, deploy the Calico CNI to the EC2 cluster, which can be automated using Argo CD.\nFetch the kubeconfig file for the newly created cluster on EC2:\nclusterctl get kubeconfig capi-ec2 \u0026gt; capikubeconfig-ec2 Deploy Calico CNI\nkubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml Calico CNI is successfully deployed, check the cluster status again\nWhen creating an EKS cluster, the system automatically generates and stores kubeconfig files as secrets within the management cluster. This behavior differs from creating a non-managed cluster using the AWS provider, where kubeconfigs are not stored in this way. The secret containing the kubeconfig will be named [cluster-name]-user-kubeconfig, where you should replace [cluster-name] with the actual name of your cluster.\nTo get the user kubeconfig for a cluster named ‘capi-eks’ you can run a command similar to:\nkubectl --namespace=default get secret capi-eks-user-kubeconfig -o jsonpath={.data.value} | base64 --decode \u0026gt; capikubeconfig-eks Cluster Upgrades Choose the target Kubernetes version (e.g., v1.32.0). Update your cluster manifest: Open your EC2 manifest file aws-ec2.yaml.\nModify the version field in KubeadmControlPlane and MachineDeployment: Ensure the MachineDeployment is configured to handle rolling updates\nspec:\rstrategy:\rtype: RollingUpdate\rrollingUpdate:\rmaxSurge: 1\rmaxUnavailable: 0 Commit and push\nCheck in argo cd UI Open your EC2 manifest file capi-eks.yaml.\nModify the version field in AWSManagedControlPlane: Commit and push Check in argo cd UI Cluster Autoscaling Autoscaling allows your clusters to dynamically adjust resources based on workload demand.\n1. EC2-Based Cluster Autoscaling Manifest Cluster Autoscaler Create file management/cluster-autoscaler.yaml:\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: cluster-autoscaler\rnamespace: kube-system\rlabels:\rapp: cluster-autoscaler\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: cluster-autoscaler\rtemplate:\rmetadata:\rlabels:\rapp: cluster-autoscaler\rannotations:\rcluster-autoscaler.kubernetes.io/safe-to-evict: \u0026#34;false\u0026#34;\rprometheus.io/scrape: \u0026#34;true\u0026#34;\rprometheus.io/port: \u0026#34;8085\u0026#34;\rspec:\rserviceAccountName: cluster-autoscaler\rcontainers:\r- name: cluster-autoscaler\rimage: registry.k8s.io/autoscaling/cluster-autoscaler:v1.32.0\rresources:\rlimits:\rcpu: 100m\rmemory: 300Mi\rrequests:\rcpu: 100m\rmemory: 300Mi\rcommand:\r- ./cluster-autoscaler\r- --cloud-provider=aws\r- --nodes=1:10:capi-ec2-md-0\r- --cluster-name=capi-ec2\r- --skip-nodes-with-local-storage=false\r- --balance-similar-node-groups\r- --expander=least-waste\r- --logtostderr=true\r- --v=4\rvolumeMounts:\r- name: ssl-certs\rmountPath: /etc/ssl/certs/ca-certificates.crt\rreadOnly: true\rsecurityContext:\rallowPrivilegeEscalation: false\rreadOnlyRootFilesystem: true\rrunAsNonRoot: true\rrunAsUser: 65534\rvolumes:\r- name: ssl-certs\rhostPath:\rpath: /etc/ssl/certs/ca-certificates.crt RBAC for autoscaler Create file management/cluster-autoscaler-rbac.yaml:\napiVersion: v1\rkind: ServiceAccount\rmetadata:\rname: cluster-autoscaler\rnamespace: kube-system\r---\rapiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRole\rmetadata:\rname: cluster-autoscaler\rrules:\r- apiGroups: [\u0026#34;\u0026#34;]\rresources: [\u0026#34;events\u0026#34;, \u0026#34;endpoints\u0026#34;, \u0026#34;pods\u0026#34;, \u0026#34;services\u0026#34;, \u0026#34;nodes\u0026#34;, \u0026#34;namespaces\u0026#34;]\rverbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;]\r- apiGroups: [\u0026#34;apps\u0026#34;]\rresources: [\u0026#34;replicasets\u0026#34;, \u0026#34;statefulsets\u0026#34;, \u0026#34;daemonsets\u0026#34;]\rverbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;]\r- apiGroups: [\u0026#34;extensions\u0026#34;]\rresources: [\u0026#34;replicasets\u0026#34;, \u0026#34;daemonsets\u0026#34;]\rverbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;]\r- apiGroups: [\u0026#34;policy\u0026#34;]\rresources: [\u0026#34;poddisruptionbudgets\u0026#34;]\rverbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;]\r- apiGroups: [\u0026#34;autoscaling.k8s.io\u0026#34;]\rresources: [\u0026#34;*\u0026#34;]\rverbs: [\u0026#34;*\u0026#34;]\r- apiGroups: [\u0026#34;cluster.x-k8s.io\u0026#34;]\rresources: [\u0026#34;machinedeployments\u0026#34;, \u0026#34;machinesets\u0026#34;, \u0026#34;machines\u0026#34;]\rverbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;patch\u0026#34;]\r---\rapiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRoleBinding\rmetadata:\rname: cluster-autoscaler\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: ClusterRole\rname: cluster-autoscaler\rsubjects:\r- kind: ServiceAccount\rname: cluster-autoscaler\rnamespace: kube-system Manifest Argo CD Application Cluster Autoscaler Create file management/argocd-autoscaler-app.yaml:\napiVersion: argoproj.io/v1alpha1\rkind: Application\rmetadata:\rname: cluster-autoscaler\rspec:\rproject: default\rsource:\rrepoURL: https://github.com/Tungdever/multi-cluster-with-clusterapi\rtargetRevision: HEAD\rpath: management\rdestination:\rserver: https://kubernetes.default.svc\rnamespace: kube-system\rsyncPolicy:\rautomated:\rprune: true\rselfHeal: true\rsyncOptions:\r- CreateNamespace=false Commit file cluster-autoscaler.yaml, cluster-autoscaler-rbac.yaml, and argocd-autoscaler-app.yaml Push on repo Apply Argo CD Application: kubectl apply -f management/argocd-autoscaler-app.yaml EKS Cluster Autoscaling Use managed node groups with autoscaling parameters: apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\rkind: AWSManagedMachinePool\rmetadata:\rname: capi-eks-pool-0\rnamespace: default\rspec:\rinstanceType: t3.medium\rscaling:\rmaxSize: 5\rminSize: 1 Alternatively, deploy Cluster Autoscaler for EKS if you\u0026rsquo;re using unmanaged node groups. "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-tools/2.2.5-kind/",
	"title": "Kind",
	"tags": [],
	"description": "",
	"content": "Install Run the following commands: curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.24.0/kind-linux-amd64\rchmod +x ./kind\rsudo install -o root -g root -m 0755 ./kind /usr/local/bin/kind Verify Installation kind version "
},
{
	"uri": "//localhost:1313/5-networking/",
	"title": "Set Up Cross-Cluster Networking",
	"tags": [],
	"description": "",
	"content": "Enable communication between clusters using Istio.\nInstall Istio for Service Mesh:\nVerify Cross-Cluster Communication:\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-tools/docker/",
	"title": "Docker",
	"tags": [],
	"description": "",
	"content": "Install Run the following commands: # Update the package index\rsudo apt-get update\r# Install required packages for HTTPS repository access\rsudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common\r# Add Docker\u0026#39;s official GPG key\rcurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\r# Add Docker\u0026#39;s official APT repository\recho \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null\r# Update the package index again\rsudo apt-get update\r# Install Docker\rsudo apt-get install -y docker-ce docker-ce-cli containerd.io\r# Start and enable Docker service\rsudo systemctl start docker\rsudo systemctl enable docker Verify Installation docker --version "
},
{
	"uri": "//localhost:1313/6-distribution/",
	"title": "Workload Distribution",
	"tags": [],
	"description": "",
	"content": "Distribute workloads across clusters for high availability and load balancing.\nDeploy Sample Application:\nDeploy a sample app (e.g., nginx) on both workload clusters: kubectl --kubeconfig=workload-cluster-1.kubeconfig apply -f nginx-deployment.yaml\rkubectl --kubeconfig=workload-cluster-2.kubeconfig apply -f nginx-deployment.yaml Set Up Multi-Cluster Load Balancing:\nUse an external DNS service or AWS Global Accelerator to distribute traffic across clusters. Example with AWS Route 53: aws route53 create-hosted-zone --name example.com --caller-reference $(date +%s) Create DNS records pointing to both clusters’ ingress controllers. Test Workload Distribution:\nAccess the application via the Route 53 DNS name and verify load balancing across clusters. "
},
{
	"uri": "//localhost:1313/7-security/",
	"title": "Implement Governance and Security",
	"tags": [],
	"description": "",
	"content": "Apply governance and security policies to ensure compliance and secure operations.\nDefine RBAC Policies:\nCreate a role for restricted access in workload clusters: kubectl --kubeconfig=workload-cluster-1.kubeconfig create role developer --verb=get,list --resource=pods\rkubectl --kubeconfig=workload-cluster-1.kubeconfig create rolebinding developer-binding --role=developer --user=dev-user Network Policies:\nApply a network policy to restrict traffic: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: restrict-access namespace: default spec: podSelector: matchLabels: app: nginx policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: role: frontend Apply to both workload clusters: kubectl --kubeconfig=workload-cluster-1.kubeconfig apply -f network-policy.yaml Enable AWS Security Features:\nEnable AWS EKS encryption for secrets: aws eks associate-encryption-config --cluster-name workload-cluster-1 --encryption-config file://encryption-config.json Use AWS IAM roles for service accounts (IRSA) to secure pod access to AWS resources. Audit Logging:\nEnable EKS control plane logging: aws eks update-cluster-config --name workload-cluster-1 --logging \u0026#39;{\u0026#34;clusterLogging\u0026#34;:[{\u0026#34;types\u0026#34;:[\u0026#34;api\u0026#34;,\u0026#34;audit\u0026#34;,\u0026#34;authenticator\u0026#34;,\u0026#34;controllerManager\u0026#34;,\u0026#34;scheduler\u0026#34;],\u0026#34;enabled\u0026#34;:true}]}\u0026#39; "
},
{
	"uri": "//localhost:1313/8-cost/",
	"title": "Cost Management",
	"tags": [],
	"description": "",
	"content": "Optimize costs for multi-cluster operations.\nTag Resources:\nTag all clusters and resources for cost tracking: aws eks tag-resource --resource-arn arn:aws:eks:us-east-1:\u0026lt;account-id\u0026gt;:cluster/workload-cluster-1 --tags Environment=Workshop,CostCenter=Training Use Spot Instances:\nModify worker node groups to use Spot Instances: eksctl create nodegroup --cluster workload-cluster-1 --spot --instance-types t3.medium Monitor Costs:\nEnable AWS Cost Explorer and create a cost allocation report for the Environment=Workshop tag. "
},
{
	"uri": "//localhost:1313/9-procedures/",
	"title": "Operational Procedures",
	"tags": [],
	"description": "",
	"content": "Define procedures for ongoing management.\nCluster Upgrades:\nUpgrade a workload cluster using Cluster API: kubectl patch cluster workload-cluster-1 --type merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;kubernetesVersion\u0026#34;:\u0026#34;v1.26.0\u0026#34;}}\u0026#39; Backup and Restore:\nUse Velero for cluster backups: velero install --provider aws --plugins velero/velero-plugin-for-aws --bucket workshop-backup\rvelero backup create workload-cluster-1-backup --include-cluster-resources=true Monitoring and Alerts:\nDeploy Prometheus and Grafana for monitoring: kubectl --kubeconfig=workload-cluster-1.kubeconfig apply -f prometheus-grafana.yaml Set up alerts for CPU/memory usage and cluster health. Scaling:\nScale worker nodes: eksctl scale nodegroup --cluster workload-cluster-1 --nodes 4 "
},
{
	"uri": "//localhost:1313/10-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "To avoid unnecessary costs, clean up all resources created during the workshop.\nTo make sure there’s no unwanted cloud cost, please clean up the environment.\nDelete the Argo CD applications: kubectl delete -f ./management/argocd-ec2-app.yaml kubectl delete -f ./management/argocd-eks-app.yaml\rkubectl delete -f ./app/eks-app/eks-nginx-deploy.yaml Delete the created clusters: kubectl delete cluster capi-eks\rkubectl delete cluster capi-ec2 Delete the created EC2 SSH key in each region: aws ec2 delete-key-pair --key-name capi-eks --region ap-southeast-2\raws ec2 delete-key-pair --key-name capi-ec2 --region us-east-1 Delete the CloudFormation Stack when running “clusterawsadm boostrap” command to create all necessary IAM resources aws cloudformation delete-stack --stack-name cluster-api-provider-aws-sigs-k8s-io --region us-east-1 Delete management cluster kind delete cluster - "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]